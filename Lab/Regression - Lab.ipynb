{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dk6lxcr\n",
    "Modify the regression scratch code in our lecture such that:\n",
    "\n",
    "- Implement early stopping in which if the absolute difference between old loss and new loss does not exceed certain threshold, we abort the learning.\n",
    "\n",
    "- Implement options for stochastic gradient descent in which we use only one sample for training.  Make sure that sample does not repeat unless all samples are read at least once already.\n",
    "\n",
    "- Put everything into class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Let's load some boston data \n",
    "# as our regression case study\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# type - Bunch\n",
    "# Bunch - dictionary of numpy data\n",
    "# boston.feature_names\n",
    "# print(boston)\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "X.shape #number of samples, number of features\n",
    "\n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows in X is the same as number of rows in y\n",
    "# because so we have yhat for all y\n",
    "assert m == y.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to standardize my data so that mean is 0, variance is 1\n",
    "# average across each feature, NOT across each sample\n",
    "# Why we need to standardize\n",
    "# Because standardizing usually allows us to reach convergence faster\n",
    "# Why -> because the values are within smaller range\n",
    "# Thus, the gradients are also within limited range, and NOT go crazy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. StandardScaler.fit(X)  #this scaler (or self) knows the mean and std so now\n",
    "# it knows how to transform data\n",
    "# 2  X = StandardScaler.transform(X)  #not in place; will return something\n",
    "\n",
    "# 1. StandardScaler.fit_transform(X) -> 1 and 2 sequentially\n",
    "\n",
    "# create an object of StandardScaler\n",
    "# StandardScaler is a class\n",
    "# scaler is called instance/object\n",
    "\n",
    "# ALMOST always, feature scale your data using normalization or standardization\n",
    "# If you assume your data is gaussian, use standardization, otherwise, you do the normalization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the appropriate size for test data\n",
    "# 70/30 (small dataset); 80/20 (medium dataset); 90/10 (large dataset);\n",
    "# why large dataset, can set test size to 10, because\n",
    "# 10% of large dataset is already enough for testing accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "assert len(X_train)  == len(y_train)\n",
    "assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the shape of X they want\n",
    "# (number of samples, number of features) --> correct shape\n",
    "# for closed form formula\n",
    "# How about the intercept\n",
    "# w0 is OUR intercept\n",
    "# what is the shape of w -->(n+1, )\n",
    "# What is the shape of intercept --->(m, 1)\n",
    "#X = [1 2 3     @  [w0\n",
    "#     1 4 6         w1\n",
    "#     1 9 1         w2 \n",
    "#     1 10 2 ] \n",
    "\n",
    "# np.ones((shape))\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "\n",
    "# concatenate the intercept based on axis=1\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "\n",
    "# np.ones((shape))\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "\n",
    "# concatenate the intercept based on axis=1\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# Step 1: Prepare your data\n",
    "# X_train, X_test have intercepts that are being concatenated to the data\n",
    "# [1, features\n",
    "#  1, features....]\n",
    "\n",
    "# making sure our X_train has same sample size as y_train\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "# initialize our w\n",
    "# We don't have to do X.shape[1] + 1 because our X_train already has the\n",
    "# intercept\n",
    "# w = theta/beta/coefficients\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "# define the learning rate\n",
    "# later on, you gonna know that it should be better to make it slowly decreasing\n",
    "# once we perform a lot of iterations, we want the update to slow down, so it converges better\n",
    "alpha = 0.0001\n",
    "\n",
    "# define our max_iter\n",
    "# typical to call it epochs <---ml people likes to call it\n",
    "max_iter = 1000\n",
    "\n",
    "loss_old = 10000\n",
    "\n",
    "tol = 0.0001\n",
    "\n",
    "iter_stop = 0\n",
    "\n",
    "\n",
    "def h_theta(X, theta):\n",
    "    return X @ theta\n",
    "\n",
    "def mse(yhat, y):\n",
    "    return ((yhat - y)**2).sum() / yhat.shape[0]\n",
    "\n",
    "def gradient(X, error):\n",
    "    return X.T @ error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implement early stopping in which if the absolute difference between old loss and new loss does not exceed certain threshold, we abort the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is decresing less than 0.0001, stop the training at epoch 714\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "# define your for loop\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    # 1. yhat = X @ w\n",
    "    # prediction\n",
    "    # yhat (m, ) = (m, n) @ (n, )\n",
    "    yhat = h_theta(X_train, theta)\n",
    "\n",
    "    # 2. error = yhat - y_train\n",
    "    # error for use to calculate gradients\n",
    "    # error (m, ) = (m, ) - (m, )\n",
    "    error = yhat - y_train\n",
    "    \n",
    "    \n",
    "    loss_current = mse(yhat, y_train)\n",
    "    if abs(loss_current - loss_old) < tol:\n",
    "        print(f'Loss is decresing less than {tol}, stop the training at epoch {i}')\n",
    "        break\n",
    "    loss_old = loss_current\n",
    "    \n",
    "    # 3. grad = X.T @ error\n",
    "    # grad (n, ) = (n, m) @ (m, )\n",
    "    # grad for each feature j\n",
    "    grad = gradient(X_train, error)\n",
    "\n",
    "    # 4. w = w - alpha * grad\n",
    "    # update w\n",
    "    # w (n, ) = (n, ) - scalar * (n, )\n",
    "    theta = theta - alpha * grad\n",
    "\n",
    "time_taken = time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement options for stochastic gradient descent in which we use only one sample for training.  Make sure that sample does not repeat unless all samples are read at least once already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 14)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# batch_size == 1 to indicate 'stochastic gradient descent'\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "# X_train = list(range(20))\n",
    "\n",
    "num_step = math.ceil(len(X_train) / batch_size)\n",
    "X_train_copy = X_train.copy()\n",
    "y_train_copy = y_train.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "# define your for loop\n",
    "for i in range(max_iter):\n",
    "    for step in range(num_step):\n",
    "        X_train = X_train_copy[step * batch_size: (step +1 ) * batch_size]\n",
    "        y_train = y_train_copy[step * batch_size: (step +1 ) * batch_size]\n",
    "        # 1. yhat = X @ w\n",
    "        # prediction\n",
    "        # yhat (m, ) = (m, n) @ (n, )\n",
    "        yhat = h_theta(X_train, theta)\n",
    "\n",
    "        # 2. error = yhat - y_train\n",
    "        # error for use to calculate gradients\n",
    "        # error (m, ) = (m, ) - (m, )\n",
    "        error = yhat - y_train\n",
    "\n",
    "        # 3. grad = X.T @ error\n",
    "        # grad (n, ) = (n, m) @ (m, )\n",
    "        # grad for each feature j\n",
    "        grad = gradient(X_train, error)\n",
    "\n",
    "        # 4. w = w - alpha * grad\n",
    "        # update w\n",
    "        # w (n, ) = (n, ) - scalar * (n, )\n",
    "        theta = theta - alpha * grad\n",
    "\n",
    "time_taken = time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we got our lovely w\n",
    "# now it's time to check our accuracy\n",
    "# 1. Make prediction\n",
    "yhat = h_theta(X_test, theta)\n",
    "\n",
    "# 2. Calculate mean squared errors\n",
    "mse_loss = mse(yhat, y_test)\n",
    "\n",
    "# print the mse\n",
    "print(\"MSE: \", mse_loss)\n",
    "print(\"Stop at iteration: \", iter_stop)\n",
    "print(\"Time used: \", time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Add options for mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# mini-batch gradient descent with batch_size == 16\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# X_train = list(range(20))\n",
    "\n",
    "num_step = math.ceil(len(X_train) / batch_size)\n",
    "X_train_copy = X_train.copy()\n",
    "y_train_copy = y_train.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "# define your for loop\n",
    "for i in range(max_iter):\n",
    "    for step in range(num_step):\n",
    "        X_train = X_train_copy[step * batch_size: (step +1 ) * batch_size]\n",
    "        y_train = y_train_copy[step * batch_size: (step +1 ) * batch_size]\n",
    "        # 1. yhat = X @ w\n",
    "        # prediction\n",
    "        # yhat (m, ) = (m, n) @ (n, )\n",
    "        yhat = h_theta(X_train, theta)\n",
    "\n",
    "        # 2. error = yhat - y_train\n",
    "        # error for use to calculate gradients\n",
    "        # error (m, ) = (m, ) - (m, )\n",
    "        error = yhat - y_train\n",
    "\n",
    "        # 3. grad = X.T @ error\n",
    "        # grad (n, ) = (n, m) @ (m, )\n",
    "        # grad for each feature j\n",
    "        grad = gradient(X_train, error)\n",
    "\n",
    "        # 4. w = w - alpha * grad\n",
    "        # update w\n",
    "        # w (n, ) = (n, ) - scalar * (n, )\n",
    "        theta = theta - alpha * grad\n",
    "\n",
    "time_taken = time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  21.625144089931087\n",
      "Stop at iteration:  0\n",
      "Time used:  0.13797378540039062\n"
     ]
    }
   ],
   "source": [
    "# we got our lovely w\n",
    "# now it's time to check our accuracy\n",
    "# 1. Make prediction\n",
    "yhat = h_theta(X_test, theta)\n",
    "\n",
    "# 2. Calculate mean squared errors\n",
    "mse_loss = mse(yhat, y_test)\n",
    "\n",
    "# print the mse\n",
    "print(\"MSE: \", mse_loss)\n",
    "print(\"Stop at iteration: \", iter_stop)\n",
    "print(\"Time used: \", time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Put everything into class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    def __init__(self,  lerning_rate = 0.00001,\n",
    "                 max_iter=10000, early_stopping=True, \n",
    "                 early_stopping_tol=0.0001, gradient_batch_type='batch', batch_size=None):\n",
    "        \n",
    "        self.lerning_rate = lerning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.early_stopping = early_stopping\n",
    "        self.early_stopping_tol = early_stopping_tol\n",
    "        self.gradient_batch_type = gradient_batch_type\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.loss_old = np.inf\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.theta = None\n",
    "        self.m = None\n",
    "        self.n = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.m = self.X.shape[0]\n",
    "        self.n = self.X.shape[1]\n",
    "        self.theta = np.zeros((self.n))\n",
    "        \n",
    "        \n",
    "        # Batch\n",
    "        if self.gradient_batch_type == 'batch' or self.batch_size == None:\n",
    "            batch_size = self.X.shape[0]\n",
    "        elif self.gradient_batch_type == 'minibatch' or self.batch_size != None:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        elif self.gradient_batch_type == 'stochastic':\n",
    "            batch_size = 1\n",
    "        \n",
    "        num_step = math.ceil(len(X_train) / batch_size)\n",
    "        X_copy = self.X.copy()\n",
    "        y_copy = self.y.copy()\n",
    "        \n",
    "        is_break = False\n",
    "        for i in range(self.max_iter):\n",
    "            for step in range(num_step):\n",
    "                self.X = X_copy[step * batch_size: (step +1 ) * batch_size]\n",
    "                self.y = y_copy[step * batch_size: (step +1 ) * batch_size]\n",
    "                # 1. yhat = X @ w\n",
    "                # prediction\n",
    "                # yhat (m, ) = (m, n) @ (n, )\n",
    "                yhat = h_theta(self.X, self.theta)\n",
    "\n",
    "                # 2. error = yhat - y_train\n",
    "                # error for use to calculate gradients\n",
    "                # error (m, ) = (m, ) - (m, )\n",
    "                error = yhat - self.y\n",
    "\n",
    "                loss_current = mse(yhat, self.y)\n",
    "\n",
    "                \n",
    "\n",
    "                # 3. grad = X.T @ error\n",
    "                # grad (n, ) = (n, m) @ (m, )\n",
    "                # grad for each feature j\n",
    "                grad = gradient(self.X, error)\n",
    "                \n",
    "\n",
    "                # 4. w = w - alpha * grad\n",
    "                # update w\n",
    "                # w (n, ) = (n, ) - scalar * (n, )\n",
    "#                 print(self.theta, self.lerning_rate * grad)\n",
    "                self.theta = self.theta - self.lerning_rate * grad\n",
    "                \n",
    "            if i % 100 == 0:\n",
    "                print(f'Epoch {i}: MSE Loss {loss_current}')\n",
    "            if abs(loss_current - self.loss_old) < self.early_stopping_tol and self.early_stopping == True:\n",
    "                print(f'Loss is decresing less than {self.early_stopping_tol}, stop the training at epoch {i}')\n",
    "                break\n",
    "            self.loss_old = loss_current\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return h_theta(X, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE Loss 617.6244632768362\n",
      "Epoch 100: MSE Loss 286.6378478151129\n",
      "Epoch 200: MSE Loss 149.51910935121103\n",
      "Epoch 300: MSE Loss 84.84701375124693\n",
      "Epoch 400: MSE Loss 53.747969572406774\n",
      "Epoch 500: MSE Loss 38.586325954910755\n",
      "Epoch 600: MSE Loss 31.09842219558615\n",
      "Epoch 700: MSE Loss 27.347885894692524\n",
      "Epoch 800: MSE Loss 25.43564713569859\n",
      "Epoch 900: MSE Loss 24.43620944623522\n",
      "Epoch 1000: MSE Loss 23.894772845800986\n",
      "Epoch 1100: MSE Loss 23.58619533519085\n",
      "Epoch 1200: MSE Loss 23.39821234640508\n",
      "Epoch 1300: MSE Loss 23.274383331516447\n",
      "Epoch 1400: MSE Loss 23.186033088944505\n",
      "Epoch 1500: MSE Loss 23.11837713120564\n",
      "Epoch 1600: MSE Loss 23.063627874506018\n",
      "Epoch 1700: MSE Loss 23.017553469587305\n",
      "Epoch 1800: MSE Loss 22.977751513293768\n",
      "Epoch 1900: MSE Loss 22.942777864119886\n",
      "Epoch 2000: MSE Loss 22.911703546496827\n",
      "Epoch 2100: MSE Loss 22.883886926734366\n",
      "Epoch 2200: MSE Loss 22.85885473539715\n",
      "Epoch 2300: MSE Loss 22.83623852196752\n",
      "Epoch 2400: MSE Loss 22.81573962080569\n",
      "Epoch 2500: MSE Loss 22.79710898908049\n",
      "Epoch 2600: MSE Loss 22.780134958826906\n",
      "Epoch 2700: MSE Loss 22.764635320306645\n",
      "Epoch 2800: MSE Loss 22.750451867480198\n",
      "Epoch 2900: MSE Loss 22.73744641237224\n",
      "Epoch 3000: MSE Loss 22.725497726992916\n",
      "Epoch 3100: MSE Loss 22.714499107523856\n",
      "Loss is decresing less than 0.0001, stop the training at epoch 3168\n"
     ]
    }
   ],
   "source": [
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)\n",
    "yhat = lin.predict(X_test)\n",
    "\n",
    "loss = mse(yhat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.383126295326274"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
